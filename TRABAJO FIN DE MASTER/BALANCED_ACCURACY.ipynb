{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xZRxYMtnjHcs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, cross_validate\n",
        "from sklearn import linear_model, tree, ensemble\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trabajo sobre los conjuntos de datos ya preparados para la predicción\n",
        "df_train = pd.read_excel(\"df_limpia.xlsx\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8STof-Vqk93t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#analizo el número de registros por clase, porque después de probar con StratifiedKFold, tenía clases con un único registro, menor que los folds establecidos.\n",
        "\n",
        "\n",
        "ytrain = df_train['service_code']\n",
        "print(ytrain.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Uk_d2PDZRou",
        "outputId": "10d022a9-9d77-4b31-9e75-8ef9d9b46941"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4849671      9265\n",
            "4849672      5835\n",
            "4849669      4738\n",
            "4849666      3396\n",
            "5144577      2784\n",
            "1000007      2366\n",
            "5144587      1679\n",
            "97550336     1278\n",
            "5144579       846\n",
            "9043968       814\n",
            "5144576       798\n",
            "1000008       741\n",
            "1000000       526\n",
            "1000010       474\n",
            "9043969       436\n",
            "1000012       420\n",
            "5144578       311\n",
            "5931009       301\n",
            "7733248       291\n",
            "1000002       247\n",
            "103677952     230\n",
            "97779712      217\n",
            "4849670       215\n",
            "5144585       206\n",
            "1000017       181\n",
            "5144582       147\n",
            "4849673       134\n",
            "118587392     134\n",
            "188088321     122\n",
            "1000029       119\n",
            "1000009       114\n",
            "1000014       112\n",
            "1000016       107\n",
            "82182147      102\n",
            "4784129        99\n",
            "5144586        75\n",
            "5472256        72\n",
            "1000011        67\n",
            "5144581        65\n",
            "1000030        65\n",
            "40000          63\n",
            "1000018        58\n",
            "6029312        44\n",
            "1000015        41\n",
            "1000028        41\n",
            "16777217       39\n",
            "4849665        33\n",
            "6520832        20\n",
            "102793216      13\n",
            "4849668        11\n",
            "24182784        9\n",
            "82182145        9\n",
            "82182144        1\n",
            "1000001         1\n",
            "16777216        1\n",
            "Name: service_code, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#me quedo para la predicción con las clases que tengan al menos 100 registros.\n",
        "print(len(df_train))\n",
        "df_train = df_train.groupby(\"service_code\").filter(lambda x: len(x) >=99)\n",
        "print(len(df_train))\n",
        "#volvemos a definir ytrain\n",
        "ytrain = df_train['service_code']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g55chzuAa6de",
        "outputId": "c1152939-a68a-412b-b8e8-8cacd8e874df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40513\n",
            "39785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texting_corpus = df_train[\"texto_tratado\"].str.split()\n",
        "\n",
        "\n",
        "#genero una lista de listas para su vectorización del conjunto train\n",
        "corpus_train=[[word for word in elem] for elem in texting_corpus ]\n",
        "print(corpus_train[0:10])\n",
        "print(len(corpus_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJDy3U62b9xX",
        "outputId": "0f4f982c-a2a5-4d14-b064-046861c7765d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['gente', 'venir', 'observar', 'excesivo', 'limpieza', 'centro', 'agua', 'llover', 'video', 'demostrar', 'necesario', 'agua', 'mayoria', 'gente', 'ciudad', 'ciudad', 'agua'], ['arreglar', 'echar', 'culpa', 'ciudadano', 'cargo', 'animal', 'sacar', 'necesidad', 'dejar', 'echar', 'verguenza', 'vecindario', 'palabra', 'limpieza', 'seguro', 'prestar', 'atencion', 'cercano', 'perro', 'llevar', 'semana', 'invitar', 'alcalde', 'pasear', 'disfrutar', 'acera', 'mobiliario', 'urbano', 'fachada', 'pis', 'continuo', 'perro', 'asqueroso', 'limpiar', 'fondo', 'multar', 'dueño', 'vecino', 'perro', 'tener', 'enviar', 'paseo'], ['bache', 'paseo', 'reparacion', 'bache'], ['bache', 'plaza', 'reparacion', 'bache'], ['bache', 'reparacion', 'bache'], ['bache', 'reparacion', 'bache'], ['unir', 'hecho', 'porqueria', 'entrada', 'peligroso', 'trabajador', 'empresa', 'hoja', 'mando', 'foto', 'hora'], ['limpiar', 'fondo', 'estar', 'plaza', 'alrededor'], ['malestar', 'vecino', 'lamentable', 'asfalto', 'grieta', 'profundo', 'suelto', 'asegurar', 'asfaltado', 'deberiar', 'tomar', 'medida', 'urgente', 'cara', 'echar', 'algun', 'tiempo', 'necesitar', 'tirado'], ['bache', 'reparacion', 'bache']]\n",
            "39785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "'''\n",
        "Obtenemos vectores para cada uno de los documentos con TfidfVectorizer que es una herramienta de vectorización de textos\n",
        "que nó solo tiene en cuenta el nñumero de veces que aparece una palabra en un documento, sino su\n",
        "peso o frecuencia a lo largo de todo el conjunto de documentos o corpus\n",
        "'''\n",
        "#construimos el modelo\n",
        "modelo_tfidf_sklearn = TfidfVectorizer(max_df=0.5, min_df=5, use_idf= False)\n",
        "\n",
        "#generamos los vectores para el conjunto train\n",
        "xtrain =modelo_tfidf_sklearn.fit_transform([' '.join(doc) for doc in corpus_train])\n"
      ],
      "metadata": {
        "id": "J7d77wzkpfLl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#establezco 5 subconjuntos de datos formados con las mismas proporciones de clases que el conjunto inicial sobre los que estudiar los scores de los modelos\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=80)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cnt = 1\n",
        "\n",
        "for train_index, test_index in kf.split(xtrain, ytrain):\n",
        "    print(f'Fold:{cnt}, Subconjunto train: {len(train_index)}, Subconjunto test:{len(test_index)}')\n",
        "    cnt+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi73XMd_Q-Je",
        "outputId": "0777c0c2-6a60-4b3a-dc2b-9534269b5c5f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold:1, Subconjunto train: 31828, Subconjunto test:7957\n",
            "Fold:2, Subconjunto train: 31828, Subconjunto test:7957\n",
            "Fold:3, Subconjunto train: 31828, Subconjunto test:7957\n",
            "Fold:4, Subconjunto train: 31828, Subconjunto test:7957\n",
            "Fold:5, Subconjunto train: 31828, Subconjunto test:7957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = cross_val_score(linear_model.LogisticRegression(random_state= 80,max_iter=10000), xtrain, ytrain,   cv= kf, scoring=\"accuracy\")\n",
        "print(f'Scores para cada subconjunto: {score}')\n",
        "print(f'Media del score: {\"{:.2f}\".format(score.mean())}')\n",
        "\n",
        "\n",
        "#en este caso utilizo el accuracy que ya decíamos que no era el score adecuado para dataset desbalanceados, para comparar con el balanced_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih-OfG1RX4vR",
        "outputId": "286327e6-511d-4ced-91d4-fb8babe6973c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores para cada subconjunto: [0.76674626 0.76586653 0.76360437 0.75970843 0.75744627]\n",
            "Media del score: 0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#ahora como métrica utilizaremos la balanced_accuracy que tiene en cuenta el peso de las clases. Vemos que el accuracy cae respecto al cálculo anterior.\n",
        "\n",
        "score = cross_val_score(linear_model.LogisticRegression(random_state= 80,max_iter=10000), xtrain, ytrain,  cv= kf, scoring=\"balanced_accuracy\")\n",
        "print(f'Scores para cada subconjunto: {score}')\n",
        "print(f'Media del score: {\"{:.2f}\".format(score.mean())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBXgLovbSGtv",
        "outputId": "b8ffbaae-97b8-4206-8e91-aa4568648f95"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores para cada subconjunto: [0.47057514 0.48742792 0.48816441 0.48442899 0.47233893]\n",
            "Media del score: 0.48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#aqui lo que voy a hacer es valorar distintos modelos con metricas adecuadas también al desbalanceo\n",
        "#incluyo la métrica balanced_accuracy_score\n",
        "\n",
        "models = [RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
        "    LinearSVC(),\n",
        "    MultinomialNB(),\n",
        "    LogisticRegression(random_state= 80,max_iter=10000),\n",
        "]\n",
        "names = [\"Random_Forest\", \"Linear_SVC\", \"Multinomial_NB\", \"Logistic_Regression\"]\n",
        "\n",
        "\n",
        "# Elegimos las métricas que vamos a utilizar\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
        "    'recall': make_scorer(recall_score, average='weighted'),\n",
        "    'f1_score': make_scorer(f1_score, average='macro')\n",
        "}\n",
        "\n",
        "for model, name in zip(models, names):\n",
        "    # Realizamos también cross-validation con las métricas escogidas\n",
        "    pprint(name)\n",
        "    pprint(cross_validate(model, xtrain, ytrain, cv=5, scoring=scoring))\n",
        "\n",
        "#Los mejores modelos, teniendo una baja accuracy ponderada son la Regresión Logística y Linear SVC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj-S2bEGidbg",
        "outputId": "219cf0e0-8a5f-4fc6-9f0a-914fe9b1aade"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Random_Forest'\n",
            "{'fit_time': array([1.9296062 , 1.93007588, 2.20624042, 2.48741317, 2.0028832 ]),\n",
            " 'score_time': array([0.32898045, 0.3468051 , 0.44497252, 0.3452189 , 0.34538531]),\n",
            " 'test_accuracy': array([0.33643333, 0.32474551, 0.31846173, 0.32864145, 0.33190901]),\n",
            " 'test_balanced_accuracy': array([0.06169634, 0.05853957, 0.05731845, 0.06023441, 0.06055291]),\n",
            " 'test_f1_score': array([0.05158015, 0.04780056, 0.04642406, 0.04869245, 0.05133773]),\n",
            " 'test_recall': array([0.33643333, 0.32474551, 0.31846173, 0.32864145, 0.33190901])}\n",
            "'Linear_SVC'\n",
            "{'fit_time': array([3.64542532, 2.32608151, 3.28913116, 2.97012734, 1.99333715]),\n",
            " 'score_time': array([0.02503061, 0.02291322, 0.02554417, 0.02516341, 0.02409792]),\n",
            " 'test_accuracy': array([0.73532738, 0.75719492, 0.76586653, 0.75480709, 0.74186251]),\n",
            " 'test_balanced_accuracy': array([0.4868317 , 0.52449768, 0.56021312, 0.51783007, 0.49055693]),\n",
            " 'test_f1_score': array([0.49761963, 0.54351092, 0.58341236, 0.53734287, 0.50460418]),\n",
            " 'test_recall': array([0.73532738, 0.75719492, 0.76586653, 0.75480709, 0.74186251])}\n",
            "'Multinomial_NB'\n",
            "{'fit_time': array([0.03067708, 0.0296998 , 0.02954626, 0.03422785, 0.02978039]),\n",
            " 'score_time': array([0.02348232, 0.02270198, 0.02426887, 0.02288365, 0.02352691]),\n",
            " 'test_accuracy': array([0.68304637, 0.67789368, 0.70038959, 0.69850446, 0.67575719]),\n",
            " 'test_balanced_accuracy': array([0.2760304 , 0.271034  , 0.30404296, 0.29573259, 0.26084195]),\n",
            " 'test_f1_score': array([0.29210786, 0.28792514, 0.32824657, 0.31952813, 0.27656223]),\n",
            " 'test_recall': array([0.68304637, 0.67789368, 0.70038959, 0.69850446, 0.67575719])}\n",
            "'Logistic_Regression'\n",
            "{'fit_time': array([25.9832561 , 23.72437358, 30.12920237, 24.24255872, 29.18323278]),\n",
            " 'score_time': array([0.06977057, 0.05510426, 0.07255864, 0.05399489, 0.06928349]),\n",
            " 'test_accuracy': array([0.7403544 , 0.74827196, 0.76046249, 0.75342466, 0.73809225]),\n",
            " 'test_balanced_accuracy': array([0.45304058, 0.4609063 , 0.49554003, 0.47188674, 0.43345758]),\n",
            " 'test_f1_score': array([0.48504938, 0.49647799, 0.53448535, 0.50927562, 0.46431777]),\n",
            " 'test_recall': array([0.7403544 , 0.74827196, 0.76046249, 0.75342466, 0.73809225])}\n"
          ]
        }
      ]
    }
  ]
}
